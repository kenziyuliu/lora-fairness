# vs V2: bf16 vs fp16
# NOTE: FSDP won't worh with PEFT
# https://github.com/pytorch/pytorch/issues/100945
# https://github.com/h2oai/h2o-llmstudio/issues/98
compute_environment: LOCAL_MACHINE
debug: false
distributed_type: FSDP
downcast_bf16: 'no'
fsdp_config:
  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
  fsdp_backward_prefetch_policy: BACKWARD_PRE
  fsdp_forward_prefetch: true
  fsdp_offload_params: false
  fsdp_sharding_strategy: 1
  fsdp_state_dict_type: FULL_STATE_DICT
  fsdp_sync_module_states: true
  fsdp_use_orig_params: true
  # fsdp_transformer_layer_cls_to_wrap: Swinv2Layer # NOTE: SwinTransformer only
machine_rank: 0
main_training_function: main
# mixed_precision: fp16
mixed_precision: bf16   # Since we always run on skampere1, just use bf16 for better precision
num_machines: 1
num_processes: 4
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
